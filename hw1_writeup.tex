\documentclass[11pt]{article}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{cancel}
\usepackage{mathtools}
\usepackage{graphicx}
\setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in}
\textwidth 6.5in
\textheight 9in
\headheight 0.0in
\topmargin -.5in
\oddsidemargin 0.0in
\evensidemargin 0.0in

\title{\bf CS 181 - Homework 1}
\date{\today}
\author{Lexi Ross \& Ye Zhao}
\begin{document}
\maketitle
\section{Decision Tree \& ID3}

\section{ID3 with Pruning}

\section{Boosting}

\subsection{Information Gain Criterion using weights}
In order to take into account weights when calculating the information gain of a given attribute, we use the following formula to calculate the entropy of the labels:
$$H(\text{labels}=\sum_{c=1}^{C}\frac{W_c}{W}\log_2\frac{W}{W_c}$$
Where $$W_c = \sum_{n=1}^{N}I(y_n=c)w_n$$ and $$W=\sum_{n=1}^{N}w_n$$

\end{document}